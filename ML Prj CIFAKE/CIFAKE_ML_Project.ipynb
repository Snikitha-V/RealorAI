{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6434deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99e48fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifake_data(data_path):\n",
    "    \"\"\"Load CIFAKE dataset from test set\"\"\"\n",
    "    real_images = []\n",
    "    fake_images = []\n",
    "    \n",
    "    # Load real images from test set\n",
    "    real_path = os.path.join(data_path, 'test', 'REAL')\n",
    "    for img_file in os.listdir(real_path):\n",
    "        img = Image.open(os.path.join(real_path, img_file))\n",
    "        real_images.append(np.array(img))\n",
    "    \n",
    "    # Load fake images from test set\n",
    "    fake_path = os.path.join(data_path, 'test', 'FAKE')\n",
    "    for img_file in os.listdir(fake_path):\n",
    "        img = Image.open(os.path.join(fake_path, img_file))\n",
    "        fake_images.append(np.array(img))\n",
    "    \n",
    "    # Create labels (0 = REAL, 1 = FAKE)\n",
    "    X = np.array(real_images + fake_images)\n",
    "    y = np.array([0] * len(real_images) + [1] * len(fake_images))\n",
    "    \n",
    "    # Normalize pixel values to 0-1 range\n",
    "    X = X.astype('float32') / 255.0\n",
    "    \n",
    "    # Flatten images for traditional ML algorithms\n",
    "    X_flattened = X.reshape(X.shape[0], -1)\n",
    "    \n",
    "    return X, X_flattened, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5eef4d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data path to CIFAKE directory\n",
    "data_path = os.path.join(os.getcwd(), 'data', 'CIFAKE')\n",
    "\n",
    "# Load and split data\n",
    "X, X_flat, y = load_cifake_data(data_path)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_flat, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# For neural networks, keep original shape\n",
    "X_train_cnn, X_test_cnn, _, _ = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc2a0ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.6555\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Implementation\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
    "print(f\"Logistic Regression Accuracy: {lr_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d289b5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Try different k values\n",
    "k_values = [3, 5, 7, 9, 11]\n",
    "best_k = 5  # You can optimize this\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=best_k)\n",
    "knn_model.fit(X_train, y_train)\n",
    "knn_pred = knn_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d8a3e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Use RBF kernel\n",
    "svm_model = SVC(kernel='rbf', random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_pred = svm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25c22c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "nb_pred = nb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba4d2b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state=42, max_depth=10)\n",
    "dt_model.fit(X_train, y_train)\n",
    "dt_pred = dt_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91d9603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "babf6ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_model = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1d14eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_model = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "ada_model.fit(X_train, y_train)\n",
    "ada_pred = ada_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65e2bad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sniki\\OneDrive\\Desktop\\RealorAI\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# Using Keras/TensorFlow\n",
    "mlp_model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(3072,)),  # 32*32*3 = 3072\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "mlp_model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "mlp_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
    "mlp_pred = (mlp_model.predict(X_test) > 0.5).astype(int).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acca16a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    \"\"\"Comprehensive evaluation function\"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa23367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "results = []\n",
    "results.append(evaluate_model(y_test, lr_pred, 'Logistic Regression'))\n",
    "results.append(evaluate_model(y_test, knn_pred, 'KNN'))\n",
    "results.append(evaluate_model(y_test, svm_pred, 'SVM'))\n",
    "results.append(evaluate_model(y_test, nb_pred, 'Naive Bayes'))\n",
    "results.append(evaluate_model(y_test, dt_pred, 'Decision Tree'))\n",
    "results.append(evaluate_model(y_test, rf_pred, 'Random Forest'))\n",
    "results.append(evaluate_model(y_test, xgb_pred, 'XGBoost'))\n",
    "results.append(evaluate_model(y_test, ada_pred, 'AdaBoost'))\n",
    "results.append(evaluate_model(y_test, mlp_pred, 'MLP'))\n",
    "\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4d9ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Results DataFrame\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL COMPARISON RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nDetailed Performance Metrics:\\n\")\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c6590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Model Accuracies\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Accuracy Comparison Bar Chart\n",
    "ax1 = axes[0, 0]\n",
    "models = results_df['Model'].values\n",
    "accuracies = results_df['Accuracy'].values\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(models)))\n",
    "bars = ax1.bar(models, accuracies, color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax1.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "for i, (bar, acc) in enumerate(zip(bars, accuracies)):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "             f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# 2. Precision Comparison\n",
    "ax2 = axes[0, 1]\n",
    "precisions = results_df['Precision'].values\n",
    "bars = ax2.bar(models, precisions, color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax2.set_ylabel('Precision', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Model Precision Comparison', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "for i, (bar, prec) in enumerate(zip(bars, precisions)):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "             f'{prec:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# 3. Recall Comparison\n",
    "ax3 = axes[1, 0]\n",
    "recalls = results_df['Recall'].values\n",
    "bars = ax3.bar(models, recalls, color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax3.set_ylabel('Recall', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Model Recall Comparison', fontsize=14, fontweight='bold')\n",
    "ax3.set_ylim([0, 1])\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "for i, (bar, rec) in enumerate(zip(bars, recalls)):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "             f'{rec:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# 4. F1-Score Comparison\n",
    "ax4 = axes[1, 1]\n",
    "f1_scores = results_df['F1-Score'].values\n",
    "bars = ax4.bar(models, f1_scores, color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax4.set_ylabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "ax4.set_title('Model F1-Score Comparison', fontsize=14, fontweight='bold')\n",
    "ax4.set_ylim([0, 1])\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "for i, (bar, f1) in enumerate(zip(bars, f1_scores)):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "             f'{f1:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "plt.setp(ax4.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualizations displayed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0023460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Model Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üèÜ BEST MODEL ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find best model by each metric\n",
    "best_accuracy_idx = results_df['Accuracy'].idxmax()\n",
    "best_precision_idx = results_df['Precision'].idxmax()\n",
    "best_recall_idx = results_df['Recall'].idxmax()\n",
    "best_f1_idx = results_df['F1-Score'].idxmax()\n",
    "\n",
    "print(f\"\\nü•á Best Accuracy: {results_df.loc[best_accuracy_idx, 'Model']}\")\n",
    "print(f\"   Score: {results_df.loc[best_accuracy_idx, 'Accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\nü•á Best Precision: {results_df.loc[best_precision_idx, 'Model']}\")\n",
    "print(f\"   Score: {results_df.loc[best_precision_idx, 'Precision']:.4f}\")\n",
    "\n",
    "print(f\"\\nü•á Best Recall: {results_df.loc[best_recall_idx, 'Model']}\")\n",
    "print(f\"   Score: {results_df.loc[best_recall_idx, 'Recall']:.4f}\")\n",
    "\n",
    "print(f\"\\nü•á Best F1-Score: {results_df.loc[best_f1_idx, 'Model']}\")\n",
    "print(f\"   Score: {results_df.loc[best_f1_idx, 'F1-Score']:.4f}\")\n",
    "\n",
    "# Overall best model (by accuracy)\n",
    "overall_best = results_df.loc[best_accuracy_idx]\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚ú® OVERALL BEST MODEL: {overall_best['Model']}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Accuracy:  {overall_best['Accuracy']:.4f}\")\n",
    "print(f\"Precision: {overall_best['Precision']:.4f}\")\n",
    "print(f\"Recall:    {overall_best['Recall']:.4f}\")\n",
    "print(f\"F1-Score:  {overall_best['F1-Score']:.4f}\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f327e774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a detailed heatmap of all metrics\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Prepare data for heatmap\n",
    "heatmap_data = results_df[['Accuracy', 'Precision', 'Recall', 'F1-Score']].values\n",
    "\n",
    "# Create heatmap\n",
    "im = ax.imshow(heatmap_data, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "\n",
    "# Set ticks and labels\n",
    "ax.set_xticks(np.arange(len(['Accuracy', 'Precision', 'Recall', 'F1-Score'])))\n",
    "ax.set_yticks(np.arange(len(results_df)))\n",
    "ax.set_xticklabels(['Accuracy', 'Precision', 'Recall', 'F1-Score'], fontweight='bold')\n",
    "ax.set_yticklabels(results_df['Model'], fontweight='bold')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label('Score', rotation=270, labelpad=20, fontweight='bold')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(results_df)):\n",
    "    for j in range(len(['Accuracy', 'Precision', 'Recall', 'F1-Score'])):\n",
    "        text = ax.text(j, i, f'{heatmap_data[i, j]:.3f}',\n",
    "                      ha=\"center\", va=\"center\", color=\"black\", fontweight='bold', fontsize=10)\n",
    "\n",
    "ax.set_title('Model Performance Heatmap - All Metrics', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Heatmap visualization displayed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aed1e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Rankings by Different Metrics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä MODEL RANKINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Rank by Accuracy\n",
    "print(\"\\nüèÜ RANKINGS BY ACCURACY:\")\n",
    "print(\"-\" * 80)\n",
    "acc_ranking = results_df.sort_values('Accuracy', ascending=False).reset_index(drop=True)\n",
    "for idx, row in acc_ranking.iterrows():\n",
    "    medal = ['ü•á', 'ü•à', 'ü•â', '4Ô∏è‚É£', '5Ô∏è‚É£', '6Ô∏è‚É£', '7Ô∏è‚É£', '8Ô∏è‚É£', '9Ô∏è‚É£'][idx]\n",
    "    print(f\"{medal} {idx+1}. {row['Model']:25} ‚Üí Accuracy: {row['Accuracy']:.4f}\")\n",
    "\n",
    "# Rank by F1-Score\n",
    "print(\"\\n\\nüèÜ RANKINGS BY F1-SCORE:\")\n",
    "print(\"-\" * 80)\n",
    "f1_ranking = results_df.sort_values('F1-Score', ascending=False).reset_index(drop=True)\n",
    "for idx, row in f1_ranking.iterrows():\n",
    "    medal = ['ü•á', 'ü•à', 'ü•â', '4Ô∏è‚É£', '5Ô∏è‚É£', '6Ô∏è‚É£', '7Ô∏è‚É£', '8Ô∏è‚É£', '9Ô∏è‚É£'][idx]\n",
    "    print(f\"{medal} {idx+1}. {row['Model']:25} ‚Üí F1-Score: {row['F1-Score']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
